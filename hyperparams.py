# here we just set defaults of the parameters.
# for actual experiments, edit experiments.py

gene_init = ""
total_init = ""

dataset_name = 'tv'
trainset = 'train'
valset = 'val'
B = 32 # batch size
N = 20499 # gene profile length
nCats = 46

do_batch_balance = True

#----------- augs -----------#
do_log = True
do_normalize = False
mult_noise_std = 5.0
add_noise_std = 0.1

#----------- net design -----------#
do_train = False

# total_init with the autogenerated name
do_resume = False

# run nothing
do_gene = False

# train nothing 
do_train_gene = False

#----------- general hypers -----------#
lr = 1e-5
do_dropout_input = False
do_dropout = False
do_debug = False
do_profile = False
do_fast_logging = True # doesn't log the graph

#----------- layers -----------#
# make tiny nets
nLayers_gene = 1

#----------- gene hypers -----------#
# none yet

#----------- mod -----------#
mod = "a"

############ slower-to-change hyperparams below here ############

# queue_capacity = 100+3*B
queue_capacity = 100+2*B

pad = "SYMMETRIC"

eps = 1e-6

## logging
log_freq_t = 100
log_freq_v = 100
snap_freq = 1000

max_iters = 10000
shuffle_train = True
shuffle_val = True

dataset_list_dir = "./data/"
dataset_location = "./data/tfrs/"
fold = 0

############ rev up the experiment ############

execfile('experiments.py')

############ make some final adjustments ############

dataset_t = "%s/%s_%d.txt" % (dataset_list_dir, trainset, fold)
dataset_v = "%s/%s_%d.txt" % (dataset_list_dir, valset, fold)

# if we're testing, turn on/off the modules asked for in the model name
if not do_train:
    do_gene = not total_init.find("G") == -1

############ autogen a name; don't touch any hypers! ############

if do_batch_balance:
    name = "%02dbb_%.1e" % (B*nCats, lr)
else:
    name = "%02d_%.1e" % (B, lr)

if do_gene:
    # name += "_G%d" % nLayers_gene
    # name += "_G%d" % nLayers_gene
    if do_log:
        name += "_log"
    if not add_noise_std==0.0:
        name += "_an%.2f" % add_noise_std
    if not mult_noise_std==0.0:
        name += "_mn%.2f" % mult_noise_std
    if do_normalize:
        name += "_norm"
        
    # if do_train_gene:
    #     name = "%s_G%d" % (name,
    #                        nLayers_gene)
    # else:
    #     name = "%s_G%d" % (name,
    #                        nLayers_gene)
    
##### end model description

if do_dropout_input:
    name += "_dropi"
if do_dropout:
    name += "_drop"

# add some training data info
name = "%s_%s%d" % (name, dataset_name, fold)
if trainset:
    name = "%s_%s" % (name, trainset)
if trainset==valset:
    # only val!
    name += "_ov"
        
if do_debug:
    name += "_debug"

if mod:
    name = "%s_%s" % (name, mod)

if do_resume:
    total_init = name

# if we're testing, forget the crazy name, just call it init_mod
# if not do_train:
#     name = "TEST_%s_%dx%d_%s_%s_%s" % (total_init, H, W, dataset_name, valset, mod)

print name
